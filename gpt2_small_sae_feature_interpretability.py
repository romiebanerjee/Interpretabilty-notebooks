# -*- coding: utf-8 -*-
"""gpt2-small-SAE-feature-interpretability.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vaaJIDyUs0hevRAO40KG0lKeKD1149Tr
"""

# !pip install transformer_lens

import numpy as np
import matplotlib.pyplot as plt
import torch
import blobfile as bf

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device: ", device)

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/romiebanerjee/sae-tools.git
# %cd sae-tools
import sparse_autoencoder as sae

import transformer_lens

model = transformer_lens.HookedTransformer.from_pretrained("gpt2", center_writing_weights=False).to(device)

def get_layer_name(location: str, layer_index:int):
    transformer_lens_loc = {
        "mlp_post_act": f"blocks.{layer_index}.mlp.hook_post",
        "resid_delta_attn": f"blocks.{layer_index}.hook_attn_out",
        "resid_post_attn": f"blocks.{layer_index}.hook_resid_mid",
        "resid_delta_mlp": f"blocks.{layer_index}.hook_mlp_out",
        "resid_post_mlp": f"blocks.{layer_index}.hook_resid_post",
    }[location]

    return transformer_lens_loc

## download gpt-2 SAE weights from Openai
layer_index = 10
location = "resid_post_mlp"

with bf.BlobFile(sae.paths.v5_128k(location, layer_index), mode="rb") as f:
    print(f"fetching weights for sae at {get_layer_name(location, layer_index)} from {sae.paths.v5_128k(location, layer_index)}")
    state_dict = torch.load(f)

import requests

# Download Frankenstein from Project Gutenberg
url = "https://www.gutenberg.org/files/84/84-0.txt"  # Frankenstein's permanent URL
response = requests.get(url)

# Save to a file
with open("frankenstein.txt", "w", encoding="utf-8") as f:
    f.write(response.text)

# Load the entire file into a string
with open('frankenstein.txt', 'r', encoding='utf-8') as file:
    text = file.read()  # Entire content as a single string
#split into smaller strings
import re
sentences = re.split(r'(?<=[.!?\n])\s+', text)
print(len(sentences))

# model.to_tokens(sentences)
batch_tokens = model.tokenizer(sentences, padding=True, return_tensors="pt")['input_ids']
batch_masks = model.tokenizer(sentences, padding=True, return_tensors="pt")['attention_mask']
print(batch_tokens.shape)
print(batch_masks.shape)

def random_row_indices(tensor, r):
    R = tensor.size(0)
    # Generate random indices without replacement
    rand_indices = torch.randperm(R)[:r]
    return rand_indices

rand_indices = random_row_indices(batch_tokens, 100)

batch_tokens = batch_tokens[rand_indices]
batch_masks = batch_masks[rand_indices]

print(batch_tokens.shape)
print(batch_masks.shape)

for i,r in enumerate(rand_indices):
  print(i, sentences[r])

layer_name = get_layer_name(location, layer_index)
with torch.no_grad():
  batch_logits, batch_activation_cache = model.run_with_cache(batch_tokens, remove_batch_dim=False)
print(batch_logits.shape)
print(batch_activation_cache[layer_name].shape)

input = "The rain pattered dismally against the panes, and my candle was out."
str_tokens = model.to_str_tokens(input)
print(str_tokens)
tokens = model.to_tokens(input)
print(tokens)
with torch.no_grad():
  logits, activation_cache = model.run_with_cache(input, remove_batch_dim=True)
print(logits.shape)
print(activation_cache[layer_name].shape)

token_index = str_tokens.index(' out')
print(token_index)
layer_name = get_layer_name(location, layer_index)

act = activation_cache[layer_name][token_index, :]

input_tensor = activation_cache[layer_name]

autoencoder = sae.Autoencoder.from_state_dict(state_dict).to(device)

with torch.no_grad():
    latent_activations, info = autoencoder.encode(input_tensor)
    reconstructed_activations = autoencoder.decode(latent_activations, info)


decoder_weights = autoencoder.decoder.weight
act_dots = act @ decoder_weights
top_feature_indx = torch.argmax(act_dots)
top_feature = decoder_weights[:, top_feature_indx]

print(f'input tensor: {input_tensor.shape}')
print(f'latent activations: {latent_activations.shape}')
print(f'reconstructed activations: {reconstructed_activations.shape}')
print(f'act_dots: {act_dots.shape}')
print(f'top_feature index: {top_feature_indx}')
print(f'top_feature: {top_feature.shape}')

# print(torch.nonzero(latent_activations[50, :]))

act @ top_feature

batch_acts = batch_activation_cache[layer_name].detach().cpu()
print(batch_acts.shape)

batch_acts_top = batch_acts @ top_feature
mask = batch_acts_top > 0.
batch_acts_top = batch_acts_top * mask
print(batch_acts_top.shape)

for line, i in enumerate(range(100)):
  print(line, list(zip(model.to_str_tokens(sentences[rand_indices[i]]), batch_acts_top[i, :].data)))
  print('---------------------------------------------------------------------------------------------------------------')

max, indx = torch.max(batch_acts_top, dim=1)

torch.max(max), sentences[rand_indices[torch.argmax(max)]]

# acts_sentences = torch.sum(batch_acts_top, dim = 1)

for i in range(100):
  print(i, list(zip((max[i].data, sentences[rand_indices[i]]))))

from IPython.display import HTML, display

def highlight_text(words, values, cmap='RdYlGn'):
    """
    Highlight words in text with color gradient based on values.
    Perfect for displaying in Colab notebooks.
    """
    # Normalize values to [0, 1] if not already
    values = np.array(values, dtype=float)
    if values.min() < 0 or values.max() > 1:
        values = (values - values.min()) / (values.max() - values.min() + 1e-8)

    # Get colormap
    cmap = plt.get_cmap(cmap)

    # Create highlighted HTML
    span = ""
    for word, val in zip(words, values):
        # Get RGBA color from colormap
        rgba = cmap(val)
        # Convert to hex
        color = '#%02x%02x%02x' % (int(rgba[0]*255), int(rgba[1]*255), int(rgba[2]*255))
        # Create span with background color
        span += f'<span style="background-color: {color}; border-radius: 3px; padding: 0 2px;">{word}</span>'
    return span


HTML(highlight_text(model.to_str_tokens(input), activation_cache[layer_name]@top_feature.detach() ))

for line, i in enumerate(range(100)):
  # print(list(zip(model.to_str_tokens(sentences[rand_indices[i]]), batch_acts_top[i, :].data)))
  # text = sentences[rand_indices[i]]
  words = model.to_str_tokens(sentences[rand_indices[i]])
  values = batch_acts_top[i, :].data

  display(HTML(f'<div style="font-size: 16px; line-height: 1.6; padding: 10px;">{highlight_text(words, values)}</div>'))
  print('------------------------------')

