## An collection of colab notebooks for various mechinterp miniprojects.

- **Measuring Superposition**: An overview of the Linear representation hypothesis and  superposition principle and a demo the SAE features for GPT-2 small. *Methods: SAE, TransformerLens* [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://drive.google.com/file/d/1sW04sDM41yjxJIDfqGjWN1yTkV17_nqi/view?usp=sharing)  [Notebook](https://github.com/romiebanerjee/Interpretability-notebooks/blob/main/superposition.ipynb)


 - **Interpreting SAE features**: A replicating methods of OpenAI and EleutherAI to generate and evaluate GPT-2 SAE feature interpretations. *Methods:Transformerlens*. [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1vaaJIDyUs0hevRAO40KG0lKeKD1149Tr?usp=sharing)  [Notebook](https://github.com/romiebanerjee/Interpretability-notebooks/blob/main/gpt2_small_SAE_interpretability.ipynb)

 - **Transformer Circuit Analysis**: A sampler for reverse-engineered attention heads in GPT-2-small to explain *Induction Heads*, replicating Anthropicâ€™s findings. *Methods: Activation patching, attention visualization (TransformerLens)*. *[notebook]()*